{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qJxdj_RvUBAqFLNBN1TE5lg3lXgg92M1",
      "authorship_tag": "ABX9TyOOBIJ/ln8n883wT+viHyO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirshahu51/RAG/blob/main/1_Building_Agentic_RAG_with_llamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing required libraries**"
      ],
      "metadata": {
        "id": "spLwaI2Xckit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePh3B6FNa9Q4",
        "outputId": "e0a39636-87ec-4fb4-a2e7-a90a6831b6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "llama-index==0.10.27\n",
        "llama-index-llms-text-generation-inference\n",
        "llama-index-llms-huggingface\n",
        "llama-index-llms-huggingface-api\n",
        "sentence-transformers #for embedding model\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-embeddings-instructor\n",
        "#llama-index-llms-openai==0.1.15\n",
        "#llama-index-embeddings-openai==0.1.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "vziuUAN8b7As",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d331d0e-2edc-4758-a448-f7f482ab6085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.6/915.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2A9RrXmfNGln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to make async work well with jupyter notebook\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "E_hCSseuh1Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Login for HuggingFace API for LLM**"
      ],
      "metadata": {
        "id": "flcXMZSScsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "key = userdata.get('HUGGING_FACE')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = key"
      ],
      "metadata": {
        "id": "1wmVESyMcO3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from typing import List, Optional\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ],
      "metadata": {
        "id": "Gr6pRu7Y8tQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946071c0-4e7b-4dc3-bf68-9acba4f98e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /usr/local/lib/python3.10/dist-\n",
            "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will use the same model, but run remotely on Hugging Face's servers,\n",
        "# accessed via the Hugging Face Inference API\n",
        "# Note that using your token will not charge you money,\n",
        "# the Inference API is free it just has rate limits\n",
        "\n",
        "remotely_run = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=key)\n",
        "\n",
        "# Or you can skip providing a token, using Hugging Face Inference API anonymously\n",
        "remotely_run_anon = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\")"
      ],
      "metadata": {
        "id": "KMzk3oIr7-OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=key)\n"
      ],
      "metadata": {
        "id": "DIot441o9my9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion_response = llm.complete(\"To infinity, and\")\n",
        "print(completion_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pr5WprV9olh",
        "outputId": "fac144ae-9e6c-4b33-a917-8402dcd42c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " beyond!\n",
            "\n",
            "The Toy Story franchise has been a beloved part of pop culture for over two decades, and it's not slowing down anytime soon. The latest installment, Toy Story 4, is set to hit theaters this summer, and it's already generating buzz.\n",
            "\n",
            "The movie follows the adventures of Woody, Buzz, and the gang as they embark on a new adventure with a new toy, Forky. The trailer for the movie has been released, and it's already getting fans excited for the film.\n",
            "\n",
            "One of the most exciting things about Toy Story 4 is the return of some beloved characters. Bo Peep, who was last seen in Toy Story 2, is back and looking better than ever. She's now a modern, independent woman, and her new look has been getting a lot of attention.\n",
            "\n",
            "Another exciting addition to the movie is the introduction of new characters, including Forky, who is voiced by Tony Hale. Forky is a spork with a popsicle stick for a handle, and he's not exactly thrilled about being a toy.\n",
            "\n",
            "The trailer for Toy Story 4 has been viewed over 10 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.llamaindex.ai/en/stable/examples/agent/return_direct_agent/\n",
        "#check examples of using tools using llama index and then apply what have learned inthe tutorial.s"
      ],
      "metadata": {
        "id": "PxPqgw3Ta6ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"For calling own deployed model for inference over huggingface.\"\"\"\n",
        "\n",
        "# import os\n",
        "# from typing import List, Optional\n",
        "\n",
        "# from llama_index.llms.text_generation_inference import (\n",
        "#     TextGenerationInference,\n",
        "# )\n",
        "\n",
        "# URL = 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha'\n",
        "# model = TextGenerationInference(\n",
        "#     model_url=URL, token=False\n",
        "# )  # set token to False in case of public endpoint\n",
        "\n",
        "# completion_response = model.complete(\"To infinity, and\")\n",
        "# print(completion_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dt5-1duL7CFA",
        "outputId": "89e92e88-a9d4-481b-f913-64cf25411b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For calling own deployed model for inference over huggingface.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.llamaindex.ai/en/stable/examples/tools/OnDemandLoaderTool/\n",
        "#https://docs.llamaindex.ai/en/stable/examples/agent/return_direct_agent/"
      ],
      "metadata": {
        "id": "_D-1UTNcewkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Router Engine"
      ],
      "metadata": {
        "id": "PORsrsNvdp6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U1Zvg82XfZ1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n"
      ],
      "metadata": {
        "id": "N-xH3hwfdJLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Documents\n",
        "documents = SimpleDirectoryReader(input_dir = \"/content/drive/MyDrive/Colab Notebooks/Data\",\n",
        "                                  required_exts= '.pdf'\n",
        "                                  ).load_data()"
      ],
      "metadata": {
        "id": "-3sVlImofcGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/MyDrive/Colab Notebooks/Data/metagpt.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "Tcba05SwfmEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqN4ZRaue2IK",
        "outputId": "0a09536a-9466-4c44-8876-57077585e2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define LLM and Embedding Model"
      ],
      "metadata": {
        "id": "gzHDy0xngzMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter  #SentenceWindowNodeParser\n",
        "splitter =  SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "nodes =  splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "hFenDRzUe5M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UBf5CVIbVo",
        "outputId": "48f34a6f-0bae-49cb-b649-b6f4d9475372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\") # Try using Stella too dunzhang/stella_en_400M_v5 https://huggingface.co/spaces/mteb/leaderboard"
      ],
      "metadata": {
        "id": "El0dRfDzZ13G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "h_Kdk75OhhMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIOl586ok6VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Summary Index and Vector Index over the same data"
      ],
      "metadata": {
        "id": "12ySqY0eAJNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "pIHJB1rWARNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Query Engines and Set Metadata"
      ],
      "metadata": {
        "id": "9p_B5efLA_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "                      response_mode = 'tree_summarize',\n",
        "                      use_async = True,\n",
        "                      verbose = True\n",
        "                      )\n",
        "\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "fAtJ_US6AofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\"Useful for summarization questions.\")\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\"Useful for retrieving specific context from documents.\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "ATZal9mNDosq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Router Query Engine"
      ],
      "metadata": {
        "id": "_7DgneKRElMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector = LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools = [summary_tool, vector_tool],\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "JMaflaY4EXau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(textwrap.fill(str(response), 100))\n",
        "#print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IkXoN1eF6eq",
        "outputId": "77840642-a0be-436b-8067-1f31650a829a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant to the question because it is useful for summarization questions..\n",
            "\u001b[0m  The document discusses the development of a new software framework called MetaGPT, which is\n",
            "designed to address the challenges of developing complex software systems using natural language\n",
            "programming. The framework consists of multiple agents, including a system agent, an architect\n",
            "agent, and a developer agent, each with their own specific functions. The system agent manages the\n",
            "overall system, the architect agent designs the system architecture, and the developer agent writes\n",
            "the code. The framework uses a global message pool and a subscription mechanism to address\n",
            "information overload, and it employs a feedback mechanism to improve the accuracy and completeness\n",
            "of the code. The framework is open-source and can be used to develop a variety of software systems,\n",
            "including brick breaker games, snake games, and recommendation engines. The framework is still in\n",
            "development and faces challenges in areas such as UI and front-end design, but it offers a more\n",
            "accessible and efficient way to program using natural language.  The document also discusses the\n",
            "development of a Python program to create an interactive weather dashboard. The program is designed\n",
            "to provide users with real-time weather updates and forecasts. The program is developed using Python\n",
            "and incorporates various weather APIs to provide accurate and up-to-date information. The program is\n",
            "also\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# took all the chunks of the entire document\n",
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yzFqh_rHVx2",
        "outputId": "fd4ee338-cdcb-4b8e-b840-f542780f52fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a7pksjQVJRYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The LLM is not giving good result.\n",
        "response = query_engine.query(\"How do agents share information with other agents?\")\n",
        "print(textwrap.fill(str(response), 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ejdtuduGOqh",
        "outputId": "e6b7301a-4e2b-494c-d756-496bc13c6863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant to the question because it is useful for summarization questions, and summarizing information about how agents share information with other agents would be a useful summary for answering this question..\n",
            "\u001b[0m  Agents share information with other agents through communication protocols, which can be\n",
            "implemented using various communication technologies such as message passing, shared memory, or\n",
            "distributed databases. The specific communication protocol used depends on the requirements of the\n",
            "system and the capabilities of the agents. In general, agents communicate with each other by\n",
            "exchanging messages that contain information about their current state, goals, and plans. These\n",
            "messages are sent over a communication channel, which can be a local or a remote channel. The\n",
            "communication channel can be implemented using various communication technologies such as TCP/IP,\n",
            "HTTP, or WebSockets. The communication protocol can also include mechanisms for handling errors,\n",
            "managing concurrency, and ensuring security and privacy. The communication protocol can be designed\n",
            "using various design patterns such as publish/subscribe, request/response, or event-driven. The\n",
            "communication protocol can also be optimized for performance, scalability, and reliability. The\n",
            "communication protocol can be tested using various testing techniques such as unit testing,\n",
            "integration testing, or end-to-end testing. The communication protocol can also be monitored using\n",
            "various monitoring techniques such as logging, tracing, or profiling. The communication protocol can\n",
            "also be maintained using various maintenance techniques such as version control, code review, or\n",
            "continuous integration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# took only relevant chunks\n",
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKeLozYaHwAi",
        "outputId": "849fcd2f-a2d0-478f-fcc0-2ee7a64392a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "51WpSYd6JiHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "#from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "#from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool, FunctionTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "def get_router_query_engine(file_path: str, llm = None, embed_model = None):\n",
        "    \"\"\"Get router query engine.\"\"\"\n",
        "    llm = llm or OpenAI(model=\"gpt-3.5-turbo\")\n",
        "    embed_model = embed_model or OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "    # load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "        llm=llm\n",
        "    )\n",
        "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            \"Useful for summarization questions related to MetaGPT\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=(\n",
        "            \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    query_engine = RouterQueryEngine(\n",
        "        selector=LLMSingleSelector.from_defaults(),\n",
        "        query_engine_tools=[\n",
        "            summary_tool,\n",
        "            vector_tool,\n",
        "        ],\n",
        "        verbose=True\n",
        "    )\n",
        "    return query_engine\n",
        "\n"
      ],
      "metadata": {
        "id": "D2i216zjVmOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from utils import get_router_query_engine\n",
        "#query_engine = get_router_query_engine(\"metagpt.pdf\") #Production grade code.\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Data/metagpt.pdf\"\n",
        "query_engine = get_router_query_engine(path, llm = llm, embed_model = embed_model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3KTd0IhJI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
        "print(textwrap.fill(str(response), 100))"
      ],
      "metadata": {
        "id": "KZyKcO7lLLNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3ab7f5-f272-4466-daf2-1d9da66d5cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant to the question because it is useful for summarization questions, which can help to provide a concise overview of the ablation study results..\n",
            "\u001b[0m  The ablation study results showed that the proposed method outperformed the baseline method in\n",
            "terms of accuracy and computational efficiency. Specifically, the proposed method achieved an\n",
            "accuracy of 98.5% on the test dataset, which is a significant improvement over the baseline method's\n",
            "accuracy of 96.2%. In terms of computational efficiency, the proposed method required only 10% of\n",
            "the computational resources required by the baseline method to achieve the same level of accuracy.\n",
            "These results demonstrate the effectiveness and efficiency of the proposed method for solving the\n",
            "problem.  In Table 3, we present the ablation study results for MetaGPT. The results demonstrate\n",
            "that adding roles can significantly improve the performance of MetaGPT. For example, adding the\n",
            "Architect role can reduce the human revision cost by 66.7%. Adding the Engineer role can reduce the\n",
            "human revision cost by 50%. Adding both roles can reduce the human revision cost by 83.3%. These\n",
            "results highlight the benefits of role-based task management in software development.  In the\n",
            "ablation study, we removed the self-referential mechanism from the MetaGPT framework and compared\n",
            "the results with the original MetaGPT framework. The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERsEdKaha55Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}