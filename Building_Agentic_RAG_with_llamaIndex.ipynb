{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qJxdj_RvUBAqFLNBN1TE5lg3lXgg92M1",
      "authorship_tag": "ABX9TyNiEqJ0fB6xVnrXg2J7JLsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirshahu51/RAG/blob/main/Building_Agentic_RAG_with_llamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing required libraries**"
      ],
      "metadata": {
        "id": "spLwaI2Xckit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePh3B6FNa9Q4",
        "outputId": "f8b3b7e5-a0d9-4612-f816-dcb37f44af24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "llama-index==0.10.27\n",
        "llama-index-llms-text-generation-inference\n",
        "llama-index-llms-huggingface\n",
        "llama-index-llms-huggingface\n",
        "sentence-transformers #for embedding model\n",
        "llama-index-embeddings-huggingface\n",
        "llama-index-embeddings-instructor\n",
        "#llama-index-llms-openai==0.1.15\n",
        "#llama-index-embeddings-openai==0.1.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "vziuUAN8b7As"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to make async work well with jupyter notebook\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "E_hCSseuh1Ka"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Login for HuggingFace API for LLM**"
      ],
      "metadata": {
        "id": "flcXMZSScsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "key = userdata.get('HUGGING_FACE')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = key"
      ],
      "metadata": {
        "id": "1wmVESyMcO3J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ],
      "metadata": {
        "id": "Gr6pRu7Y8tQP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will use the same model, but run remotely on Hugging Face's servers,\n",
        "# accessed via the Hugging Face Inference API\n",
        "# Note that using your token will not charge you money,\n",
        "# the Inference API is free it just has rate limits\n",
        "\n",
        "remotely_run = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=key)\n",
        "\n",
        "# Or you can skip providing a token, using Hugging Face Inference API anonymously\n",
        "remotely_run_anon = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\")"
      ],
      "metadata": {
        "id": "KMzk3oIr7-OK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=key)\n"
      ],
      "metadata": {
        "id": "DIot441o9my9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion_response = llm.complete(\"To infinity, and\")\n",
        "print(completion_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pr5WprV9olh",
        "outputId": "428d96af-384a-42f4-9864-cf9e588e5a95"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " beyond!\n",
            "\n",
            "The Toy Story franchise has been a beloved part of pop culture for over two decades, and it's not slowing down anytime soon. The latest installment, Toy Story 4, is set to hit theaters this summer, and it's already generating buzz.\n",
            "\n",
            "The movie follows the adventures of Woody, Buzz, and the gang as they embark on a new adventure with a new toy, Forky. The trailer for the movie has been released, and it's already getting fans excited for the film.\n",
            "\n",
            "One of the most exciting things about Toy Story 4 is the return of some beloved characters. Bo Peep, who was last seen in Toy Story 2, is back and looking better than ever. She's now a modern, independent woman, and her new look has been getting a lot of attention.\n",
            "\n",
            "Another exciting addition to the movie is the introduction of new characters, including Forky, who is voiced by Tony Hale. Forky is a spork with a popsicle stick for a handle, and he's not exactly thrilled about being a toy.\n",
            "\n",
            "The trailer for Toy Story 4 has been viewed over 10 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.llamaindex.ai/en/stable/examples/agent/return_direct_agent/\n",
        "#check examples of using tools using llama index and then apply what have learned inthe tutorial.s"
      ],
      "metadata": {
        "id": "PxPqgw3Ta6ia"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"For calling own deployed model for inference over huggingface.\"\"\"\n",
        "\n",
        "# import os\n",
        "# from typing import List, Optional\n",
        "\n",
        "# from llama_index.llms.text_generation_inference import (\n",
        "#     TextGenerationInference,\n",
        "# )\n",
        "\n",
        "# URL = 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha'\n",
        "# model = TextGenerationInference(\n",
        "#     model_url=URL, token=False\n",
        "# )  # set token to False in case of public endpoint\n",
        "\n",
        "# completion_response = model.complete(\"To infinity, and\")\n",
        "# print(completion_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dt5-1duL7CFA",
        "outputId": "fce3263e-0a53-4c54-ce6a-88ef362d6491"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For calling own deployed model for inference over huggingface.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.llamaindex.ai/en/stable/examples/tools/OnDemandLoaderTool/\n",
        "#https://docs.llamaindex.ai/en/stable/examples/agent/return_direct_agent/"
      ],
      "metadata": {
        "id": "_D-1UTNcewkY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Router Engine"
      ],
      "metadata": {
        "id": "PORsrsNvdp6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U1Zvg82XfZ1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n"
      ],
      "metadata": {
        "id": "N-xH3hwfdJLm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Documents\n",
        "documents = SimpleDirectoryReader(input_dir = \"/content/drive/MyDrive/Colab Notebooks/Data\",\n",
        "                                  required_exts= '.pdf'\n",
        "                                  ).load_data()"
      ],
      "metadata": {
        "id": "-3sVlImofcGB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/MyDrive/Colab Notebooks/Data/metagpt.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "Tcba05SwfmEG"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqN4ZRaue2IK",
        "outputId": "2f881774-1984-4aff-e369-c9cbcf235bd0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define LLM and Embedding Model"
      ],
      "metadata": {
        "id": "gzHDy0xngzMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter  #SentenceWindowNodeParser\n",
        "splitter =  SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "nodes =  splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "hFenDRzUe5M1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UBf5CVIbVo",
        "outputId": "19be118f-3146-4082-f672-fd76f3894952"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\") # Try using Stella too dunzhang/stella_en_400M_v5 https://huggingface.co/spaces/mteb/leaderboard"
      ],
      "metadata": {
        "id": "h_Kdk75OhhMe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIOl586ok6VU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Summary Index and Vector Index over the same data"
      ],
      "metadata": {
        "id": "12ySqY0eAJNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "pIHJB1rWARNU"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Query Engines and Set Metadata"
      ],
      "metadata": {
        "id": "9p_B5efLA_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "                      response_mode = 'tree_summarize',\n",
        "                      use_async = True,\n",
        "                      verbose = True\n",
        "                      )\n",
        "\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "fAtJ_US6AofF"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\"Useful for summarization questions.\")\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\"Useful for retrieving specific context from documents.\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "ATZal9mNDosq"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Router Query Engine"
      ],
      "metadata": {
        "id": "_7DgneKRElMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector = LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools = [summary_tool, vector_tool],\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "JMaflaY4EXau"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IkXoN1eF6eq",
        "outputId": "626c4922-0d7d-4490-ba15-22d6cc91cfbe"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant to the question because it is useful for summarization questions..\n",
            "\u001b[0m\n",
            "\n",
            "The document discusses the development of a new software framework called MetaGPT, which utilizes a multi-agent system to improve code generation. The framework incorporates structured communication, role-based task management, and executable feedback mechanisms to enhance collaboration and efficiency. The authors present experimental results demonstrating the superior performance of MetaGPT compared to other approaches in public benchmarks and a self-generated dataset. The framework offers functions for software engineering tasks and can handle complex and specialized development tasks efficiently. The document also discusses potential future work, including the use of multi-agent reinforcement learning and communication protocols to improve collaboration between agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# took all the chunks of the entire document\n",
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yzFqh_rHVx2",
        "outputId": "e8503130-a33d-411a-8492-9efacb5656ca"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a7pksjQVJRYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The LLM is not giving good result.\n",
        "response = query_engine.query(\"How do agents share information with other agents?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ejdtuduGOqh",
        "outputId": "dad57beb-df29-4989-d41e-ae83034b0a41"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: Choice 1 is most relevant to the question because it is useful for summarization questions, and summarizing information about how agents share information with other agents would be a useful way to answer this question..\n",
            "\u001b[0m\n",
            "\n",
            "1. Agents share information through a centralized database.\n",
            "2. Agents share information through a decentralized network.\n",
            "3. Agents share information through a combination of centralized and decentralized methods.\n",
            "\n",
            "---------------------\n",
            "Given the information from multiple sources and prior knowledge, answer the query.\n",
            "Query: How do agents share information with other agents?\n",
            "Answer: \n",
            "\n",
            "1. Agents share information through a centralized database.\n",
            "2. Agents share information through a decentralized network.\n",
            "3. Agents share information through a combination of centralized and decentralized methods.\n",
            "\n",
            "---------------------\n",
            "Given the information from multiple sources and prior knowledge, answer the query.\n",
            "Query: How do agents share information with other agents?\n",
            "Answer: \n",
            "\n",
            "1. Agents share information through a centralized database.\n",
            "2. Agents share information through a decentralized network.\n",
            "3. Agents share information through a combination of centralized and decentralized methods.\n",
            "\n",
            "---------------------\n",
            "Given the information from multiple sources and prior knowledge, answer the query.\n",
            "Query: How do agents share information with other agents?\n",
            "Answer: \n",
            "\n",
            "1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# took only relevant chunks\n",
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKeLozYaHwAi",
        "outputId": "3422c160-181c-455a-fc8d-7bdf0f9c378d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "51WpSYd6JiHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_router_query_engine\n",
        "\n",
        "query_engine = get_router_query_engine(\"metagpt.pdf\") #Production grade code.\n",
        "\n"
      ],
      "metadata": {
        "id": "L3KTd0IhJI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "KZyKcO7lLLNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}